import pickle

# Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ ØªØµÙˆÛŒØ±ÛŒ
with open("C:\\Users\\ariyan\\AI\\image_features.pkl", "rb") as f:
    image_features = pickle.load(f)

# Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ú©Ù¾Ø´Ù†â€ŒÙ‡Ø§
with open("C:\\Users\\ariyan\\AI\\captions.pkl", "rb") as f:
    captions = pickle.load(f)

print(f"âœ… Loaded {len(image_features)} image features and {len(captions)} captions.")

# TODO: Ø§Ø¯Ø§Ù…Ù‡ Ú©Ø¯ Ø¨Ø±Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ùˆ Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ LSTM Ø±Ø§ Ø§Ø¶Ø§ÙÙ‡ Ú©Ù†ÛŒØ¯...

from tensorflow.keras.preprocessing.text import Tokenizer


# Ø¨Ø±Ø±Ø³ÛŒ Ù…Ù‚Ø¯Ø§Ø± `all_captions`
all_captions = list(captions.values())
print(f"âœ… Total captions: {len(all_captions)}")
print("ğŸ” Sample captions:", all_captions[:5])  # Ù†Ù…Ø§ÛŒØ´ Ûµ Ú©Ù¾Ø´Ù† Ø§ÙˆÙ„

# Ø³Ø§Ø®Øª ØªÙˆÚ©Ù†Ø§ÛŒØ²Ø±
tokenizer = Tokenizer(num_words=10000, oov_token="UNK")
tokenizer.fit_on_texts(all_captions)

# Ø¨Ø±Ø±Ø³ÛŒ ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„Ù…Ø§Øª Ø¯Ø± ØªÙˆÚ©Ù†Ø§ÛŒØ²Ø±
print(f"âœ… Total words in tokenizer: {len(tokenizer.word_index)}")

# Ø°Ø®ÛŒØ±Ù‡ ØªÙˆÚ©Ù†Ø§ÛŒØ²Ø± Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø¨Ø¹Ø¯ÛŒ
with open("C:\\Users\\ariyan\\AI\\tokenizer.pkl", "wb") as f:
    pickle.dump(tokenizer, f)

print(f"âœ… Tokenizer created with {len(tokenizer.word_index)} words.")



from keras.preprocessing.sequence import pad_sequences
import numpy as np

# ØªØ¹ÛŒÛŒÙ† Ø·ÙˆÙ„ Ø­Ø¯Ø§Ú©Ø«Ø±ÛŒ Ø¬Ù…Ù„Ø§Øª
max_length = max(len(caption.split()) for caption in all_captions)

# ØªØ¨Ø¯ÛŒÙ„ Ú©Ù¾Ø´Ù†â€ŒÙ‡Ø§ Ø¨Ù‡ ØªÙˆØ§Ù„ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ
X_train, y_train, image_input = [], [], []

for key, caption in captions.items():  # Ø­Ø§Ù„Ø§ ÙÙ‚Ø· ÛŒÚ© Ú©Ù¾Ø´Ù† Ø¨Ø±Ø§ÛŒ Ù‡Ø± ØªØµÙˆÛŒØ± Ø¯Ø§Ø±ÛŒÙ…
    image_id = key.split(".")[0] + ".jpg"  # Ø­Ø°Ù Ù¾Ø³ÙˆÙ†Ø¯ Ø§Ø¶Ø§ÙÛŒ

    if image_id not in image_features:
        print(f"âš  Skipping {image_id} (not found in features)")
        continue  # Ø§ÛŒÙ† ØªØµÙˆÛŒØ± Ø±Ùˆ Ù†Ø§Ø¯ÛŒØ¯Ù‡ Ø¨Ú¯ÛŒØ±

    seq = tokenizer.texts_to_sequences([caption])[0]
    if len(seq) < 2:  # Ø§Ú¯Ø± Ú©Ù¾Ø´Ù† Ø®ÛŒÙ„ÛŒ Ú©ÙˆØªØ§Ù‡ Ø¨ÙˆØ¯ØŒ Ø±Ø¯Ø´ Ú©Ù†
        print(f"âš  Skipping empty sequence for caption: {caption}")
        continue

    for i in range(1, len(seq)):
        X_train.append(seq[:i])
        y_train.append(seq[i])
        image_input.append(image_features[image_id])

print(f"âœ… Final dataset size: X_train={len(X_train)}, y_train={len(y_train)}, image_input={len(image_input)}")


# Ø§Ø¹Ù…Ø§Ù„ Ù¾Ø¯ÛŒÙ†Ú¯ Ø¨Ø±Ø§ÛŒ ÛŒÚ©Ø³Ø§Ù†â€ŒØ³Ø§Ø²ÛŒ Ø·ÙˆÙ„ ÙˆØ±ÙˆØ¯ÛŒâ€ŒÙ‡Ø§
X_train = pad_sequences(X_train, maxlen=max_length, padding='post')
y_train = np.array(y_train)
image_input = np.array(image_input).squeeze()

print(f"âœ… Training data prepared: {X_train.shape}, {y_train.shape}, {image_input.shape}")




from keras.layers import Input, Embedding, LSTM, Dense, Dropout, Add
from keras.models import Model

# ÙˆØ±ÙˆØ¯ÛŒ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ ØªØµÙˆÛŒØ±ÛŒ
image_input_layer = Input(shape=(2048,))
image_features = Dense(256, activation="relu")(image_input_layer)

# ÙˆØ±ÙˆØ¯ÛŒ Ú©Ù¾Ø´Ù†â€ŒÙ‡Ø§
caption_input = Input(shape=(max_length,))
caption_embedding = Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=256, mask_zero=True)(caption_input)
caption_lstm = LSTM(256)(caption_embedding)

# ØªØ±Ú©ÛŒØ¨ Ø§Ø·Ù„Ø§Ø¹Ø§Øª ØªØµÙˆÛŒØ±ÛŒ Ùˆ Ù…ØªÙ†ÛŒ
merged = Add()([image_features, caption_lstm])
merged = Dense(256, activation="relu")(merged)
merged = Dropout(0.3)(merged)
output = Dense(len(tokenizer.word_index)+1, activation="softmax")(merged)

# Ø³Ø§Ø®Øª Ù…Ø¯Ù„
model = Model(inputs=[image_input_layer, caption_input], outputs=output)
model.compile(loss="sparse_categorical_crossentropy", optimizer="adam")

print("âœ… Model created successfully!")



model.fit([image_input, X_train], y_train, epochs=10, batch_size=64, verbose=1)

# Ø°Ø®ÛŒØ±Ù‡ Ù…Ø¯Ù„
model.save("C:\\Users\\ariyan\\AI\\image_captioning_model.keras")
print("âœ… Model trained and saved successfully!")

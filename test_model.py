import os
import numpy as np
import pickle
from keras.models import load_model
from keras.preprocessing.sequence import pad_sequences
import heapq  # Ø¨Ø±Ø§ÛŒ Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Beam Search

# ---------------------------
# Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„ Ø¢Ù…ÙˆØ²Ø´â€ŒØ¯ÛŒØ¯Ù‡
# ---------------------------
model_path = r"C:\Users\ariyan\AI\image_captioning_model.keras"
model = load_model(model_path)
print(f"âœ… Loaded model from {model_path}")

# ---------------------------
# Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ØªÙˆÚ©Ù†Ø§ÛŒØ²Ø±
# ---------------------------
tokenizer_path = r"C:\Users\ariyan\AI\tokenizer.pkl"
with open(tokenizer_path, "rb") as f:
    tokenizer = pickle.load(f)
print(f"âœ… Loaded tokenizer from {tokenizer_path}")

# ---------------------------
# Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ ØªØµÙˆÛŒØ±ÛŒ
# ---------------------------
features_path = r"C:\Users\ariyan\AI\image_features.pkl"
with open(features_path, "rb") as f:
    image_features = pickle.load(f)
print(f"âœ… Loaded image features from {features_path}")

# ---------------------------
# Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ù‚Ø¯Ø§Ø± max_length
# ---------------------------
max_length_path = r"C:\Users\ariyan\AI\max_length.pkl"
if os.path.exists(max_length_path):
    with open(max_length_path, "rb") as f:
        max_length = pickle.load(f)
    print(f"ğŸ“ Loaded max_length: {max_length}")
else:
    max_length = 33  # Ù…Ù‚Ø¯Ø§Ø± Ù¾ÛŒØ´â€ŒÙØ±Ø¶ Ø¯Ø± ØµÙˆØ±Øª Ø¹Ø¯Ù… ÙˆØ¬ÙˆØ¯ ÙØ§ÛŒÙ„
    print(f"ğŸ“ max_length file not found. Using default max_length: {max_length}")

# ---------------------------
# ØªÙ†Ø¸ÛŒÙ… Ø§Ù†Ø¯ÛŒØ³ ØªÙˆÚ©Ù† UNK (Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² Ø§Ù†ØªØ®Ø§Ø¨ Ø¢Ù† Ø¯Ø± Beam Search)
# ---------------------------
unk_index = tokenizer.word_index.get("UNK", None)
if unk_index is None:
    # Ø§Ú¯Ø± Ø¨Ù‡ Ù‡Ø± Ø¯Ù„ÛŒÙ„ÛŒ ØªÙˆÚ©Ù† "UNK" Ø¯Ø± tokenizer ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯ØŒ Ù…Ù‚Ø¯Ø§Ø± Ù¾ÛŒØ´â€ŒÙØ±Ø¶ ØªØ¹ÛŒÛŒÙ† Ù…ÛŒâ€ŒØ´ÙˆØ¯.
    unk_index = -1

# ---------------------------
# ØªØ§Ø¨Ø¹ ØªÙˆÙ„ÛŒØ¯ Ú©Ù¾Ø´Ù† Ø¨Ø§ Beam Search (Ø¨Ù‡Ø¨ÙˆØ¯ ÛŒØ§ÙØªÙ‡)
# ---------------------------
def beam_search_caption(features, beam_width=5):
    """ØªÙˆÙ„ÛŒØ¯ Ú©Ù¾Ø´Ù† Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Beam Search Ø¨Ø§ length normalization"""
    start_seq = [tokenizer.word_index.get("startseq", 1)]
    sequences = [(start_seq, 0.0)]
    
    for _ in range(max_length):
        all_candidates = []
        for seq, score in sequences:
            sequence_input = pad_sequences([seq], maxlen=max_length, padding="post")
            y_pred = model.predict([features, sequence_input], verbose=0)[0]
            top_indices = np.argsort(y_pred)[-beam_width:]
            for idx in top_indices:
                if idx == tokenizer.word_index.get("UNK", 0):
                    continue
                candidate = seq + [idx]
                # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² length normalization: ØªÙ‚Ø³ÛŒÙ… Ù†Ù…Ø±Ù‡ Ø¨Ø± Ø·ÙˆÙ„ ØªÙˆØ§Ù„ÛŒ
                candidate_score = (score - np.log(y_pred[idx] + 1e-10)) / float(len(candidate))
                all_candidates.append((candidate, candidate_score))
                
        if not all_candidates:
            break
        sequences = heapq.nsmallest(beam_width, all_candidates, key=lambda tup: tup[1])
        if all(seq[-1] == tokenizer.word_index.get("endseq", -1) for seq, _ in sequences):
            break
            
    best_seq = sequences[0][0]
    caption = [tokenizer.index_word.get(idx, "") for idx in best_seq]
    caption = [word for word in caption if word not in ["startseq", "endseq", "UNK", ""]]
    return " ".join(caption)


# ---------------------------
# ØªØ§Ø¨Ø¹ ØªÙˆÙ„ÛŒØ¯ Ú©Ù¾Ø´Ù† Ø¨Ø±Ø§ÛŒ ÛŒÚ© ØªØµÙˆÛŒØ±
# ---------------------------
def generate_caption(image_path, beam_width=3):
    """
    ØªÙˆÙ„ÛŒØ¯ Ú©Ù¾Ø´Ù† Ø¨Ø±Ø§ÛŒ ÛŒÚ© ØªØµÙˆÛŒØ± Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø§Ø² Ù‚Ø¨Ù„ Ø§Ø³ØªØ®Ø±Ø§Ø¬â€ŒØ´Ø¯Ù‡.
    ØªÙˆØ¬Ù‡: Ù†Ø§Ù… ÙØ§ÛŒÙ„ (Ø¨Ø¯ÙˆÙ† Ù…Ø³ÛŒØ±) Ø¨Ø§ÛŒØ¯ Ø¯Ù‚ÛŒÙ‚Ø§ Ø¨Ø§ Ú©Ù„ÛŒØ¯Ù‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯ Ø¯Ø± image_features Ù…Ø·Ø§Ø¨Ù‚Øª Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ø¯.
    """
    image_name = os.path.basename(image_path)
    if image_name not in image_features:
        print(f"âŒ Error: Image {image_name} not found in image_features.pkl")
        return ""
    
    features = image_features[image_name]  # Ø¯Ø±ÛŒØ§ÙØª ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ ØªØµÙˆÛŒØ±
    return beam_search_caption(features, beam_width=beam_width)

# ---------------------------
# ØªØ³Øª Ù…Ø¯Ù„ Ø±ÙˆÛŒ ÛŒÚ© ØªØµÙˆÛŒØ± Ø¬Ø¯ÛŒØ¯
# ---------------------------
test_image_path = r"C:\Users\ariyan\AI\test\667626_18933d713e.jpg"  # Ù…Ø³ÛŒØ± ØªØµÙˆÛŒØ± ØªØ³ØªÛŒØ› Ù…Ø·Ù…Ø¦Ù† Ø´ÙˆÛŒØ¯ Ú©Ù‡ Ù†Ø§Ù… Ø¢Ù† Ø¨Ø§ Ú©Ù„ÛŒØ¯Ù‡Ø§ÛŒ image_features Ù‡Ù…Ø®ÙˆØ§Ù†ÛŒ Ø¯Ø§Ø±Ø¯.
generated_caption = generate_caption(test_image_path, beam_width=5)
print(f"ğŸ–¼ï¸ Generated Caption for {test_image_path}: {generated_caption}")
